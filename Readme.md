# Project Title

Multimodal Rumour Detect
## Dataset

Here is the link to our MULTIMODAL PHEME - 2016 DATASET: [Google Drive Link](https://drive.google.com/file/d/1XR7g6UL8_4yqvo12alQn2iqmWvHb6iKr/view)

## Paper

You can cite our work using the following BibTeX entry:

```bibtex
@InProceedings{10.1007/978-3-031-41682-8_15,
  author = {Kumar, Raghvendra and Sinha, Ritika and Saha, Sriparna and Jatowt, Adam},
  editor = {Fink, Gernot A. and Jain, Rajiv and Kise, Koichi and Zanibbi, Richard},
  title = {Multimodal Rumour Detection: Catching News that Never Transpired!},
  booktitle = {Document Analysis and Recognition - ICDAR 2023},
  year = {2023},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  pages = {231--248},
  abstract = {"The growth of unverified multimodal content on microblogging sites has emerged as a challenging problem in recent times. One major roadblock to this problem is the unavailability of automated tools for rumour detection. Previous work in this field mainly involves rumour detection for textual content only. As per recent studies, the incorporation of multiple modalities (text and image) is provably useful in many tasks since it enhances the understanding of the context. This paper introduces a novel multimodal architecture for rumour detection. It consists of two attention-based BiLSTM neural networks for the generation of text and image feature representations, fused using a cross-modal fusion block and ultimately passing through the rumour detection module. To establish the efficiency of the proposed approach, we extend the existing PHEME-2016 data set by collecting available images and in case of non-availability, additionally downloading new images from the Web. Experiments show that our proposed architecture outperforms state-of-the-art results by a large margin."},
  isbn = {978-3-031-41682-8}
}

